{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "\n",
    "n_workers = max(os.cpu_count() - 1, 1)\n",
    "client = Client(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Iterable, Literal\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "class NLDIClient:\n",
    "    \"\"\"Client tool to retrieve data from the Network Linked Data Index API.\"\"\"\n",
    "    base_url: str = \"https://labs.waterdata.usgs.gov/api/nldi\"\n",
    "\n",
    "    def get_data(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        data_handler: Callable[[str], pd.DataFrame],\n",
    "        parameters: Optional[dict[str, str]] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generic data retrieval method.\"\"\"\n",
    "        url = self.base_url + endpoint\n",
    "        if parameters:\n",
    "            url += \"?\" + \"&\".join([f\"{k}={v}\" for k, v in parameters.items()])\n",
    "        return data_handler(url)\n",
    "\n",
    "    def get_data_sources(self) -> pd.DataFrame:\n",
    "        \"\"\"Get list of data sources.\"\"\"\n",
    "        return self.get_data(\"/linked-data\", pd.read_json)\n",
    "\n",
    "    def get_registered_feature(self, feature_source: str, feature_id: str) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Get site information.\"\"\"\n",
    "        return self.get_data(f\"/linked-data/{feature_source}/{feature_id}\", gpd.read_file)\n",
    "\n",
    "    def get_basin(self, feature_source: str, feature_id: str, simplified: bool, split_catchment: bool) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Get upstream catchment boundary.\"\"\"\n",
    "        return self.get_data(\n",
    "            f\"/linked-data/{feature_source}/{feature_id}/basin\",\n",
    "            gpd.read_file,\n",
    "            parameters = {\n",
    "                \"simplified\": str(simplified).lower(),\n",
    "                \"splitCatchment\": str(split_catchment).lower()\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class RetroConfiguration:\n",
    "    NWM_VERSION: str\n",
    "    VARIABLE_NAME: str\n",
    "    START_DATE: datetime\n",
    "    END_DATE: datetime\n",
    "    LOCATION_IDS: Iterable[int]\n",
    "    OUTPUT_DIR: Path\n",
    "    CHUNK_BY: Literal[\"week\", \"month\", \"year\"]\n",
    "\n",
    "@dataclass\n",
    "class USGSConfiguration:\n",
    "    START_DATE: datetime\n",
    "    END_DATE: datetime\n",
    "    SITES: list[str]\n",
    "    OUTPUT_DIR: Path\n",
    "    CHUNK_BY: Literal[\"week\", \"month\", \"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teehr.loading.nwm.retrospective_points as nwm_retro\n",
    "from teehr.loading.usgs import usgs\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def load_retrospective_points(\n",
    "    config: RetroConfiguration,\n",
    ") -> pd.DataFrame:\n",
    "    if config.OUTPUT_DIR.exists():\n",
    "        return dd.read_parquet(config.OUTPUT_DIR).compute()\n",
    "        \n",
    "    nwm_retro.nwm_retro_to_parquet(\n",
    "        nwm_version=config.NWM_VERSION,\n",
    "        variable_name=config.VARIABLE_NAME,\n",
    "        start_date=config.START_DATE,\n",
    "        end_date=config.END_DATE,\n",
    "        location_ids=config.LOCATION_IDS,\n",
    "        output_parquet_dir=config.OUTPUT_DIR,\n",
    "        chunk_by=config.CHUNK_BY\n",
    "    )\n",
    "    return dd.read_parquet(config.OUTPUT_DIR).compute()\n",
    "\n",
    "def load_usgs_points(\n",
    "    config: USGSConfiguration,\n",
    ") -> pd.DataFrame:\n",
    "    if config.OUTPUT_DIR.exists():\n",
    "        return dd.read_parquet(config.OUTPUT_DIR).compute()\n",
    "        \n",
    "    usgs.usgs_to_parquet(\n",
    "        sites = config.SITES,\n",
    "        start_date=config.START_DATE,\n",
    "        end_date=config.END_DATE,\n",
    "        output_parquet_dir = config.OUTPUT_DIR,\n",
    "        chunk_by = config.CHUNK_BY\n",
    "    )\n",
    "    return dd.read_parquet(config.OUTPUT_DIR).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_site_code = \"02146470\"\n",
    "\n",
    "nldi_client = NLDIClient()\n",
    "site_info = nldi_client.get_registered_feature(\"nwissite\", f\"USGS-{usgs_site_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_config = RetroConfiguration(\n",
    "    NWM_VERSION = \"nwm30\",\n",
    "    VARIABLE_NAME = \"streamflow\",\n",
    "    START_DATE = nwm_retro.NWM30_MIN_DATE,\n",
    "    END_DATE = nwm_retro.NWM30_MAX_DATE,\n",
    "    LOCATION_IDS = site_info.comid.astype(int).values,\n",
    "    OUTPUT_DIR = Path().home() / f\"temp/USGS-{usgs_site_code}-NWM-V30\",\n",
    "    CHUNK_BY = \"year\"\n",
    ")\n",
    "\n",
    "sim = load_retrospective_points(retro_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_config = USGSConfiguration(\n",
    "    START_DATE = nwm_retro.NWM30_MIN_DATE,\n",
    "    END_DATE = nwm_retro.NWM30_MAX_DATE,\n",
    "    SITES = [usgs_site_code],\n",
    "    OUTPUT_DIR = Path().home() / f\"temp/USGS-{usgs_site_code}-OBS\",\n",
    "    CHUNK_BY = \"year\"\n",
    ")\n",
    "\n",
    "obs = load_usgs_points(usgs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydrotools.events.event_detection import decomposition as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event detection works on single pandas.Series with a DatetimeIndex\n",
    "obs_ts = obs[[\"value_time\", \"value\"]].drop_duplicates().set_index(\"value_time\").resample(\"1h\").nearest(limit=1).ffill().bfill()[\"value\"]\n",
    "sim_ts = sim[[\"value_time\", \"value\"]].drop_duplicates().set_index(\"value_time\").resample(\"1h\").nearest(limit=1).ffill().bfill()[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect events\n",
    "events = ev.list_events(\n",
    "    obs_ts,\n",
    "    halflife='6h', \n",
    "    window='7D',\n",
    "    minimum_event_duration='6h',\n",
    "    start_radius='6h'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterize events\n",
    "events['obs_peak'] = events.apply(\n",
    "    lambda e: obs_ts.loc[e.start:e.end].max(), \n",
    "    axis=1\n",
    "    )\n",
    "events['sim_peak'] = events.apply(\n",
    "    lambda e: sim_ts.loc[e.start:e.end].max(), \n",
    "    axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error metrics\n",
    "events[\"peak_bias\"] = events[\"sim_peak\"].sub(events[\"obs_peak\"]).div(events[\"obs_peak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>obs_peak</th>\n",
       "      <th>sim_peak</th>\n",
       "      <th>peak_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-10-09 12:00:00</td>\n",
       "      <td>1986-10-12 05:00:00</td>\n",
       "      <td>1.189308</td>\n",
       "      <td>2.74</td>\n",
       "      <td>1.303862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-10-13 09:00:00</td>\n",
       "      <td>1986-10-16 16:00:00</td>\n",
       "      <td>3.114853</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.467164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-10-25 11:00:00</td>\n",
       "      <td>1986-10-28 20:00:00</td>\n",
       "      <td>1.529110</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.282956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986-11-01 07:00:00</td>\n",
       "      <td>1986-11-04 17:00:00</td>\n",
       "      <td>0.792872</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.281094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-11-05 06:00:00</td>\n",
       "      <td>1986-11-06 23:00:00</td>\n",
       "      <td>0.031149</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4.457721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                start                 end  obs_peak  sim_peak  peak_bias\n",
       "0 1986-10-09 12:00:00 1986-10-12 05:00:00  1.189308      2.74   1.303862\n",
       "1 1986-10-13 09:00:00 1986-10-16 16:00:00  3.114853      4.57   0.467164\n",
       "2 1986-10-25 11:00:00 1986-10-28 20:00:00  1.529110      5.02   2.282956\n",
       "3 1986-11-01 07:00:00 1986-11-04 17:00:00  0.792872      0.57  -0.281094\n",
       "4 1986-11-05 06:00:00 1986-11-06 23:00:00  0.031149      0.17   4.457721"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.bootstrap import IIDBootstrap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = IIDBootstrap(events[\"peak_bias\"])\n",
    "ci = bs.conf_int(np.median, 1000, method='percentile').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Median Peak Bias: 1.37 (1.23 to 1.52) 95% confidence'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Median Peak Bias: {events['peak_bias'].median():.2f} ({ci[0]:.2f} to {ci[1]:.2f}) 95% confidence\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
