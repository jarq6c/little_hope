{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dask.distributed import Client\n",
    "\n",
    "n_workers = max(os.cpu_count() - 1, 1)\n",
    "client = Client(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Iterable, Literal\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "class NLDIClient:\n",
    "    \"\"\"Client tool to retrieve data from the Network Linked Data Index API.\"\"\"\n",
    "    base_url: str = \"https://labs.waterdata.usgs.gov/api/nldi\"\n",
    "\n",
    "    def get_data(\n",
    "        self,\n",
    "        endpoint: str,\n",
    "        data_handler: Callable[[str], pd.DataFrame],\n",
    "        parameters: Optional[dict[str, str]] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Generic data retrieval method.\"\"\"\n",
    "        url = self.base_url + endpoint\n",
    "        if parameters:\n",
    "            url += \"?\" + \"&\".join([f\"{k}={v}\" for k, v in parameters.items()])\n",
    "        return data_handler(url)\n",
    "\n",
    "    def get_data_sources(self) -> pd.DataFrame:\n",
    "        \"\"\"Get list of data sources.\"\"\"\n",
    "        return self.get_data(\"/linked-data\", pd.read_json)\n",
    "\n",
    "    def get_registered_feature(self, feature_source: str, feature_id: str) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Get site information.\"\"\"\n",
    "        return self.get_data(f\"/linked-data/{feature_source}/{feature_id}\", gpd.read_file)\n",
    "\n",
    "    def get_basin(self, feature_source: str, feature_id: str, simplified: bool, split_catchment: bool) -> gpd.GeoDataFrame:\n",
    "        \"\"\"Get upstream catchment boundary.\"\"\"\n",
    "        return self.get_data(\n",
    "            f\"/linked-data/{feature_source}/{feature_id}/basin\",\n",
    "            gpd.read_file,\n",
    "            parameters = {\n",
    "                \"simplified\": str(simplified).lower(),\n",
    "                \"splitCatchment\": str(split_catchment).lower()\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class RetroConfiguration:\n",
    "    NWM_VERSION: str\n",
    "    VARIABLE_NAME: str\n",
    "    START_DATE: datetime\n",
    "    END_DATE: datetime\n",
    "    LOCATION_IDS: Iterable[int]\n",
    "    OUTPUT_DIR: Path\n",
    "    CHUNK_BY: Literal[\"week\", \"month\", \"year\"]\n",
    "\n",
    "@dataclass\n",
    "class USGSConfiguration:\n",
    "    START_DATE: datetime\n",
    "    END_DATE: datetime\n",
    "    SITES: list[str]\n",
    "    OUTPUT_DIR: Path\n",
    "    CHUNK_BY: Literal[\"week\", \"month\", \"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teehr.loading.nwm.retrospective_points as nwm_retro\n",
    "from teehr.loading.usgs import usgs\n",
    "import dask.dataframe as dd\n",
    "\n",
    "def load_retrospective_points(\n",
    "    config: RetroConfiguration,\n",
    ") -> pd.DataFrame:\n",
    "    if config.OUTPUT_DIR.exists():\n",
    "        return dd.read_parquet(config.OUTPUT_DIR).compute()\n",
    "        \n",
    "    nwm_retro.nwm_retro_to_parquet(\n",
    "        nwm_version=config.NWM_VERSION,\n",
    "        variable_name=config.VARIABLE_NAME,\n",
    "        start_date=config.START_DATE,\n",
    "        end_date=config.END_DATE,\n",
    "        location_ids=config.LOCATION_IDS,\n",
    "        output_parquet_dir=config.OUTPUT_DIR,\n",
    "        chunk_by=config.CHUNK_BY\n",
    "    )\n",
    "    return dd.read_parquet(config.OUTPUT_DIR).compute()\n",
    "\n",
    "def load_usgs_points(\n",
    "    config: USGSConfiguration,\n",
    ") -> pd.DataFrame:\n",
    "    if config.OUTPUT_DIR.exists():\n",
    "        return dd.read_parquet(config.OUTPUT_DIR).compute()\n",
    "        \n",
    "    usgs.usgs_to_parquet(\n",
    "        sites = config.SITES,\n",
    "        start_date=config.START_DATE,\n",
    "        end_date=config.END_DATE,\n",
    "        output_parquet_dir = config.OUTPUT_DIR,\n",
    "        chunk_by = config.CHUNK_BY\n",
    "    )\n",
    "    return dd.read_parquet(config.OUTPUT_DIR).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_site_code = \"02146470\"\n",
    "\n",
    "nldi_client = NLDIClient()\n",
    "site_info = nldi_client.get_registered_feature(\"nwissite\", f\"USGS-{usgs_site_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_config = RetroConfiguration(\n",
    "    NWM_VERSION = \"nwm30\",\n",
    "    VARIABLE_NAME = \"streamflow\",\n",
    "    START_DATE = nwm_retro.NWM30_MIN_DATE,\n",
    "    END_DATE = nwm_retro.NWM30_MAX_DATE,\n",
    "    LOCATION_IDS = site_info.comid.astype(int).values,\n",
    "    OUTPUT_DIR = Path().home() / f\"temp/USGS-{usgs_site_code}-NWM-V30\",\n",
    "    CHUNK_BY = \"year\"\n",
    ")\n",
    "\n",
    "sim = load_retrospective_points(retro_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_config = USGSConfiguration(\n",
    "    START_DATE = nwm_retro.NWM30_MIN_DATE,\n",
    "    END_DATE = nwm_retro.NWM30_MAX_DATE,\n",
    "    SITES = [usgs_site_code],\n",
    "    OUTPUT_DIR = Path().home() / f\"temp/USGS-{usgs_site_code}-OBS\",\n",
    "    CHUNK_BY = \"year\"\n",
    ")\n",
    "\n",
    "obs = load_usgs_points(usgs_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydrotools.events.event_detection import decomposition as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ts = obs[[\"value_time\", \"value\"]].drop_duplicates().set_index(\"value_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydrotools.events.event_detection._version import __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_432012/322842294.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Detect events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m events = ev.list_events(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mobs_ts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhalflife\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'6H'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'7D'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/little_hope/teehr-events/env/lib/python3.10/site-packages/hydrotools/events/event_detection/decomposition.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(series, halflife, window, minimum_event_duration, start_radius)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mend\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mindicate\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mboundaries\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \"\"\"\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# Detect event flows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     event_points = mark_event_flows(series, halflife, window, \n\u001b[0m\u001b[1;32m    332\u001b[0m         minimum_event_duration, start_radius)\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# Return events\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/little_hope/teehr-events/env/lib/python3.10/site-packages/hydrotools/events/event_detection/decomposition.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(series, halflife, window, minimum_event_duration, start_radius)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Detrend with a minimum filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mdetrended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetrend_streamflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalflife\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m# Generate mask of non-zero detrended flows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mevent_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetrended\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/little_hope/teehr-events/env/lib/python3.10/site-packages/hydrotools/events/event_detection/decomposition.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(series, halflife, window)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_duplicates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"series index has duplicate timestamps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Check values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Series contains null values.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# Smooth series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/little_hope/teehr-events/env/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Detect events\n",
    "events = ev.list_events(\n",
    "    obs_ts,\n",
    "    halflife='6H', \n",
    "    window='7D',\n",
    "    minimum_event_duration='6H',\n",
    "    start_radius='6H'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
